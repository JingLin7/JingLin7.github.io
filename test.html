<!DOCTYPE html>
<html>
<head>
  <link rel="stylesheet" type="text/css" href="styles.css">
</head>
<body>
  <head>
    <link rel="stylesheet" type="text/css" href="styles.css">
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <title>Jing Lin 林靖</title>
    <link href="./css/style.css" rel="stylesheet" media="all" type="text/css" />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css"
    />
    <script
      type="text/javascript"
      src="https://code.jquery.com/jquery-2.2.0.min.js"
    ></script>

    <script
      src="https://kit.fontawesome.com/57fb8d417e.js"
      crossorigin="anonymous"
    ></script>

    <script type="text/javascript">
      window.onload = choosePic;
      var myPix = new Array("./about/jinglin.jpg");
      function choosePic() {
        var randomNum = Math.floor(Math.random() * myPix.length);
        document.getElementById("myPicture").src = myPix[randomNum];
      }
      function lastUpdate() {
        var x = document.lastModified.substr(0, 10);
        document.getElementById("demo").innerHTML = x;
      }
    </script>
    <script type="text/javascript" src="./js/hidebib.js"></script>
    <script type="text/javascript" src="./js/loadtxt.js"></script>
  </head>
  
  <div class="container">
    <main>
      <section id="portfolio">
        <div class="project">
          <body onload="lastUpdate()">
            <div class="content">
              <div id="container">
                <table>
                  <tbody>
                    <tr>
                      <td>
                        <img
                          id="myPicture"
                          src="./about/jinglin.jpeg"
                          style="
                            float: left;
                            margin-top: -30px;
                            margin-left: 70px;
                            border-radius: 10%;
                          "
                          width="170px"
                        />
                      </td>
                      <td>
                        <div id="textContainer">
                            <h1>Jing Lin 林靖</h1>
                            <a
                              style="color: black"
                              href="https://ps.is.tuebingen.mpg.de/"
                              >Tsinghua University</a
                            ><br />
                            <br />
                            <a href="mailto: yuliang.xiu@tue.mpg.de">
                              <span
                                class="fa fa-envelope"
                                style="color: navy; font-size: 20px"
                              >
                                <span style="font-family: Optima Bold"
                                  >yuliang.xiu</span
                                >
                                <span
                                  class="fa fa-at"
                                  style="color: navy; font-size: 20px"
                                  ><span style="font-family: Optima Bold">
                                    tue.mpg.de</span
                                  ></span
                                ></span
                              ></a
                            >
                            <br />
                          </div>
                          <br />
                          <ul class="icon-list">
                            <a style="color: black" href="./about/yuliangxiu.pdf">
                              <span class="ai ai-cv fa-xl"></span>
                            </a>
        
                            <a
                              style="color: black"
                              href="https://scholar.google.com.hk/citations?hl=zh-CN&user=9zAA9rQAAAAJ"
                            >
                              <span class="ai ai-google-scholar fa-xl"></span>
                            </a>
        
                            <a
                              style="color: black"
                              href="https://github.com/yuliangxiu"
                            >
                              <span class="fa-brands fa-github fa-xl"></span>
                            </a>
        
                            <a
                              style="color: black"
                              href="https://huggingface.co/Yuliang"
                            >
                              <img
                                id="huggingface"
                                src="./about/huggingface.svg"
                                width="25px"
                              />
                            </a>
        
                            <!-- <a
                              style="color: black"
                              href="https://www.researchgate.net/profile/Yuliang-Xiu"
                            >
                              <span class="ai ai-researchgate fa-xl"></span>
                            </a> -->
                            <span style="color: gray; font-weight: normal">|</span>
                            <!-- <a
                              style="color: black"
                              href="https://www.linkedin.com/in/yuliangxiu"
                            >
                              <span class="fa fa-linkedin fa-xl"></span>
                            </a> -->
        
                            <a
                              style="color: black"
                              href="https://twitter.com/yuliangxiu"
                            >
                              <span class="fa-brands fa-twitter fa-xl"></span>
                            </a>
        
                            <!-- <a
                              style="color: black"
                              href="https://www.facebook.com/xiuyuliang1993"
                            >
                              <span class="fa fa-facebook fa-xl"></span>
                            </a> -->
        
                            <a
                              style="color: black"
                              href="https://www.zhihu.com/people/yuliangxiu"
                            >
                              <span class="fa-brands fa-zhihu fa-xl"></span>
                            </a>
        
                            <a
                              style="color: black"
                              href="https://www.youtube.com/channel/UCicL0Co86tGbzoV2heWiEaA"
                            >
                              <span class="fa-brands fa-youtube fa-xl"></span>
                            </a>
        
                            <a
                              style="color: black"
                              href="https://space.bilibili.com/86857008"
                            >
                              <span class="fa-brands fa-bilibili fa-xl"></span>
                            </a>
                          </ul>
        
                          <br />
                        </div>
                        <br />
                      </td>
                    </tr>
                  </tbody>
                </table>
                <table>
                  <tr>
                    <td>
                      <br />
                      <h1>Biography</h1>
        
                      I am currently a 3rd-Year CS Ph.D. student, working in
                      <a href="https://ps.is.tuebingen.mpg.de/">Perceiving Systems</a>,
                      <strong
                        >Max Planck Institute for Intelligent Systems (MPI-IS)</strong
                      >, with
                      <a href="https://ps.is.tuebingen.mpg.de/person/black"
                        >Michael J. Black</a
                      >. As a Marie Sklodowska-Curie Fellow (MSCA) of
                      <a href="https://www.clipe-itn.eu/">CLIPE (ESR 11)</a> project, my
                      Ph.D. topic is
                      <i
                        >Image/video-based holistic modeling for the animation of body,
                        face, hands, feet, clothing, and interaction</i
                      >.
                      <!-- which is part of the
                      <a href="https://www.clipe-itn.eu/">CLIPE</a> initiative and
                      funded by the European Union’s Horizon 2020 Research and
                      Innovation Programme.  -->
                      <br /><br />
        
                      I was briefly enrolled (2019-2020) as a Ph.D. student in
                      <a href="http://vgl.ict.usc.edu">Vision and Graphics Lab</a> at
                      <strong>University of Southern California</strong>, advised by
                      <a href="http://hao-li.com">Hao Li</a> (CEO & Co-founder of
                      Pinscreen, Associate Professor of MBZUAI). I got M.Sc. degree at
                      <strong>Shanghai Jiao Tong University</strong> in 2019, working in
                      <a href="http://mvig.sjtu.edu.cn/member/index.html"
                        >Machine Vision and Intelligence Group</a
                      >, advised by
                      <a href="http://mvig.sjtu.edu.cn/index.html">Cewu Lu</a>, and got
                      B.Eng. degree in Digital Media Technology at
                      <strong>Shandong University</strong>
                      in 2016, worked closely with
                      <a href="https://wanglusdu.github.io/">Lu Wang</a>.
        
                      <br /><br />
                      I spent wonderful time at
                      <strong>USC Institute for Creative Technology</strong>
                      with
                      <a href="https://shunsukesaito.github.io/">Shunsuke Saito</a>
                      (Research Scientist at Meta Reality Lab) and
                      <a href="https://junxnui.github.io/">Jun Xing</a> (Lead Researcher
                      at miHoYo), at <strong>Kwai Y-tech Graphics AI</strong> with
                      <a href="http://www.chongyangma.com">Chongyang Ma</a>, at
                      <strong>LightChaser Animation Studio</strong> with
                      <a href="https://zhxx1987.github.io/">Xinxin Zhang</a> (Co-founder
                      & CTO of Zenus), and worked as R&D intern at
                      <strong>Ubisoft La Forge</strong>.<br />
                    </td>
                  </tr>
                </table>
        
                <div class="logo">
                  <a href="https://is.mpg.de/person/yxiu"
                    ><img src="./about/mpi_logo.png"
                  /></a>
                  <a href="https://vgl.ict.usc.edu/people.php"
                    ><img src="./about/usc_logo.png"
                  /></a>
                  <a href="https://www.mvig.org/member/"
                    ><img src="./about/sjtu_logo.png"
                  /></a>
                  <a href="https://en.sdu.edu.cn/"
                    ><img src="./about/sdu_logo.png"
                  /></a>
                  <!-- <a href="https://www.ncku.edu.tw/"
                    ><img src="./about/ncku_logo.png"
                  /></a> -->
                  <a
                    href="https://montreal.ubisoft.com/en/our-engagements/research-and-development/"
                    ><img src="./about/ubisoft_logo.png"
                  /></a>
                  <!-- <a href="http://www.chongyangma.com/"><img
                        src="./about/kwai_logo.png"></a> -->
                  <a href="http://www.zhuiguang.com/?lang=en"
                    ><img src="./about/lightchaser_logo.png"
                  /></a>
                </div>
        
                As a <strong>Digital Avatar Researcher</strong>, I am focused on
                exploring the realms of pixels and voxels, revolving around the nexus of
                computer vision and computer graphics (CV & CG), particularly in
                relation to <strong>3D human digitization</strong> (including
                reconstruction, modeling, and animation) and the
                <strong>estimation and tracking of 2D human poses.</strong><br /><br />
        
                <strong style="color: red"
                  >I am seeking for a long-term (~6 months) internship opportunity in
                  2024 (<a href="http://127.0.0.1:5501/about/yuliangxiu.pdf">CV</a>,
                  <a href="mailto: yuliang.xiu@tue.mpg.de">Email</a>), see my works at
                  <a
                    href="https://www.youtube.com/playlist?list=PLOZbJrPfocDrDr3qKvHQBtFzYl-qrkRQc"
                  >
                    <img
                      style="vertical-align: top"
                      src="https://img.shields.io/youtube/channel/views/UCicL0Co86tGbzoV2heWiEaA?logo=youtube&labelColor=ce4630&style=flat-plastic"
                    /> </a
                ></strong>
        
                <!-- <a href="https://www.youtube.com/channel/UCicL0Co86tGbzoV2heWiEaA"
                  ><img
                    src="https://img.shields.io/youtube/channel/views/UCicL0Co86tGbzoV2heWiEaA?logo=youtube&labelColor=ce4630&style=flat-plastic"
                /></a> -->
                <br /><br />
        
                <h1>News</h1>
                [2023/01-06] Invited
                <a
                  href="https://www.bilibili.com/video/BV1NM4y1B7UN/?spm_id_from=333.999.list.card_archive.click&vd_source=4fa43a1b25f1451c4212f214517d8932"
                  ><i class="fa fa-video" aria-hidden="true">&nbsp;</i>Talk&nbsp;(30
                  min)</a
                >
                <em>"Towards Large-scale Human Digitization: Implicit or Explicit?"</em>
                (<a
                  href="https://www.dropbox.com/s/7ncqw60nkr8g7ud/ECON%26ICON.pdf?dl=0"
                  >slides</a
                >, 75MB) at <strong>industry</strong> (Taichi, Shanghai AI Lab, BIGAI,
                Huawei) and <strong>academia</strong> (PKU, CUHK, UCLA, ETH Zurich, SDU,
                CAS). <br />
                [2023/04/23] ECON won
                <strong style="color: brown">Outstanding Poster (4/80)</strong> in China
                3DV. <br />
                [2023/04/16] Now both
                <a
                  href="https://huggingface.co/spaces/Yuliang/ECON"
                  style="padding-left: 0.5rem; vertical-align: top"
                  ><img
                    src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-ECON-orange"
                /></a>
                and
                <a
                  href="https://colab.research.google.com/drive/1YRgwoRCZIrSB2e7auEWFyG10Xzjbrbno?usp=sharing"
                  style="padding-left: 0.5rem; vertical-align: top"
                  ><img
                    src="https://colab.research.google.com/assets/colab-badge.svg"
                    alt="Google Colab"
                /></a>
                are available for ECON users!<br />
                [2023/03/21] ECON has been selected as
                <strong style="color: brown">CVPR highlight</strong> papers (10% of
                accepted papers, 2.5% of submissions). <br />
                [2023/03/15] 💕
                <strong
                  >I'm thrilled to announce that Cathy and I are finally tying the knot!
                  💍
                </strong>
                <a style="color: brown" href="./about/love.jpeg"
                  ><strong>Love Moment</strong></a
                ><br />
                [2023/02/27] <a href="https://xiuyuliang.cn/econ">ECON</a> and
                <a href="https://tingtingliao.github.io/CAR/">CAR</a> got accepted by
                <strong style="color: brown">CVPR 2023</strong>
                <iframe
                  src="https://ghbtns.com/github-btn.html?user=yuliangxiu&repo=ECON&type=star&count=true&v=2&size=small"
                  frameborder="0"
                  scrolling="0"
                  width="100"
                  height="20"
                ></iframe
                ><br />
                [2023/02/13]
                <a
                  href="https://www.nytimes.com/interactive/2023/02/13/sports/football/kadarius-toney-punt-return-super-bowl-chiefs.html"
                  ><em
                    ><strong style="color: brown">NYTimes</strong>"See How Kansas City
                    Secured Its Comeback"</em
                  ></a
                >, ICON for Super Bowl 2023!<br />
                [2023/01/12] Multiple contributors support
                <a
                  href="https://github.com/YuliangXiu/ECON/blob/master/docs/installation-windows.md"
                  >Windows</a
                >,
                <a
                  href="https://github.com/YuliangXiu/ECON/blob/master/docs/installation-docker.md"
                  >Docker</a
                >,
                <a href="https://carlosedubarreto.gumroad.com/l/CEB_ECON">Blender</a>
                and<a
                  href="https://colab.research.google.com/drive/1YRgwoRCZIrSB2e7auEWFyG10Xzjbrbno?usp=sharing"
                  style="padding-left: 0.5rem"
                  >Google Colab</a
                >
                for ECON, bravo!<br />
                [2022/12/20]
                <a
                  href="https://rd.nytimes.com/projects/modeling-key-world-cup-moments-with-machine-learning"
                  ><em
                    ><strong style="color: brown">NYTimes</strong>"Modeling Key World
                    Cup Moments with Machine Learning"</em
                  ></a
                >, ICON for World Cup 2022!<br />
                <a href="javascript:toggleblock(&#39;old_news&#39;)"
                  >---- show more ----</a
                >
                <div id="old_news" style="display: none">
                  [2022/12/15] <a href="https://icon.is.tue.mpg.de/">ICON</a> belongs to
                  the past, <a href="https://xiuyuliang.cn/econ">ECON</a> is the
                  future!<br />
                  [2022/11/07]
                  <a href="https://arxiv.org/abs/2211.03375">AlphaPose</a> finally got
                  accepted by <strong style="color: brown">TPAMI 2022</strong>
                  <iframe
                    src="https://ghbtns.com/github-btn.html?user=MVIG-SJTU&repo=AlphaPose&type=star&count=true&v=2&size=small"
                    frameborder="0"
                    scrolling="0"
                    width="200"
                    height="20"
                  ></iframe
                  ><br />
                  [2022/09/17] <a href="https://dart2022.github.io/">DART</a> got
                  accepted by <strong style="color: brown">NeurIPS 2022</strong>
                  <i style="color: brown">- Datasets and Benchmarks Track</i>.
                  <iframe
                    src="https://ghbtns.com/github-btn.html?user=DART2022&repo=DART&type=star&count=true&v=2&size=small"
                    frameborder="0"
                    scrolling="0"
                    width="100"
                    height="20"
                  ></iframe
                  ><br />
                  [2022/08/29] I am invited to give a talk at <b>Adobe's</b> Digital
                  Human Seminar.<br />
                  [2022/08/01]
                  <a
                    href="https://huggingface.co/spaces/Yuliang/ICON"
                    style="padding-left: 0.5rem"
                    ><img
                      src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-ICON-orange"
                  /></a>
                  <a
                    href="https://colab.research.google.com/drive/1-AWeWhPvCTBX0KfMtgtMk10uPU05ihoA?usp=sharing"
                    style="padding-left: 0.5rem"
                    ><img
                      src="https://colab.research.google.com/assets/colab-badge.svg"
                      alt="Google Colab"
                  /></a>
                  of ICON, play with it on your images!<br />
        
                  [2022/04/20] Invited to talk about
                  <a href="https://www.buzzsprout.com/1914034/10466744">ICON</a> at
                  Talking Papers Podcast. Great chat with
                  <a href="https://www.itzikbs.com/">Yizhak Ben-Shabat</a>!<br />
                  [2022/04/07] 走进马克思普朗克智能系统研究所与苏黎世联邦理工AIT团队
                  <a
                    href="https://app6ca5octe2206.pc.xiaoe-tech.com/page/1827300?navIndex=3"
                    ><i class="fa fa-video" aria-hidden="true">&nbsp;</i
                    >视频(中文)&nbsp;</a
                  >[<a href="https://jmq.xet.tech/s/8he6q">上</a>,
                  <a href="https://jmq.xet.tech/s/ld2pb">下</a>]. <br />
                  [2022/03/02] <a href="https://icon.is.tue.mpg.de/">ICON</a> got
                  accepted by <strong style="color: brown">CVPR 2022</strong> &#8594;
                  <a href="https://readpaper.com/paper/4569785684533977089"
                    ><i class="fas fa-comments" aria-hidden="true">&nbsp;</i
                    >Reader&nbsp;</a
                  ><a href="https://discord.gg/Vqa7KBGRyk"
                    ><i class="fab fa-discord" aria-hidden="true">&nbsp;</i
                    >Discord&nbsp;</a
                  ><br />
                  [2022/02/01] I am invited to give Tech Talks by <b>USC-ICT</b>, and
                  <b>Meta Reality Labs, Pittsburgh</b> (<a
                    href="https://www.dropbox.com/s/bymd8zryce6cckf/ICON-ICT%26Meta.pdf?dl=0"
                    >PDF</a
                  >, 21MB).<br />
                  [2021/12/17] New work "ICON: Implicit Clothed humans Obtained from
                  Normals", please check its
                  <a href="https://icon.is.tue.mpg.de/">page</a>.<br />
                  [2020/09/01] Get admitted to
                  <a href="http://clipe-itn.eu/">CLIPE ESR11</a> and will be joining
                  <a href="https://ps.is.mpg.de/person/black">Michael Black</a>'s team
                  at MPI for Intelligent Systems.<br />
                  [2020/08/26]
                  <a href="https://youtu.be/THxYxcEnKFk"
                    >" Volumetric Human Teleportation"</a
                  >
                  won <strong style="color: orangered">Best in Show Award </strong>of
                  SIGGRAPH Real-Time Live 2020!<br />
                  [2020/08/25]
                  <a href="https://project-splinter.github.io/">Project-Splinter</a>:
                  Human Digitization with Implicit Representation is launched!<br />
                  [2020/07/02] Our "Monoport: Monocular Real-Time Volumetric
                  Teleportation" work was accepted by
                  <strong style="color: brown">ECCV 2020</strong>
                  <br />
                  [2020/05/08] Our "Volumetric Human Teleportation" demo was accepted by
                  <strong style="color: brown">SIGGRAPH Real-Time Live 2020</strong>
                  <br />
                  [2019/03/03] I will be joining USC CS Ph.D. Program in fall 2019,
                  advised by Hao Li <br />
                  [2019/01/24] I gave an invited
                  <a
                    href="https://ps.is.tuebingen.mpg.de/talks/pose-trajectory-extraction-and-novel-view-synthesis-from-visual-content"
                    >talk</a
                  >
                  at MPI for Intelligent Systems, Perceiving Systems department.
                  <br />
                </div>
                <br /><br />
        
                <h1 id="citation">Publications</h1>
        
                Favorite papers are
                <span style="background-color: lightgoldenrodyellow">highlighted</span>.
        
                <br />
                <table class="pub_table">
                  <tr id="xiu2023econ" class="focus">
                    <td class="pub_td1">
                      <img src="./about/econ.gif" />
                    </td>
                    <td class="pub_td2">
                      <b
                        >ECON: Explicit Clothed humans Optimized via Normal
                        integration&nbsp;</b
                      ><iframe
                        src="https://ghbtns.com/github-btn.html?user=yuliangxiu&repo=ECON&type=star&count=true&v=2&size=small"
                        frameborder="0"
                        scrolling="0"
                        width="100"
                        height="20"
                      ></iframe
                      ><br />
                      <a
                        href="https://huggingface.co/spaces/Yuliang/ECON"
                        style="padding-left: 0.5rem; vertical-align: top"
                        ><img
                          style="margin-top: 0.5em"
                          src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-ECON-orange"
                      /></a>
                      <a
                        href="https://colab.research.google.com/drive/1YRgwoRCZIrSB2e7auEWFyG10Xzjbrbno?usp=sharing"
                        style="padding-left: 0.5rem; vertical-align: top"
                        ><img
                          style="margin-top: 0.5em"
                          src="https://colab.research.google.com/assets/colab-badge.svg"
                          alt="Google Colab"
                      /></a>
                      <a
                        href="https://github.com/YuliangXiu/ECON/blob/master/docs/installation-windows.md"
                      >
                        <img
                          style="
                            height: 20px;
                            outline: 1px solid grey;
                            border-radius: 5pt;
                            margin-top: 0.5em;
                            margin-left: 0.5em;
                          "
                          src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/0a/Unofficial_Windows_logo_variant_-_2002%E2%80%932012_%28Multicolored%29.svg/1161px-Unofficial_Windows_logo_variant_-_2002%E2%80%932012_%28Multicolored%29.svg.png"
                        />
                      </a>
                      <a
                        href="https://github.com/YuliangXiu/ECON/blob/master/docs/installation-docker.md"
                      >
                        <img
                          style="
                            height: 20px;
                            outline: 1px solid grey;
                            border-radius: 5pt;
                            margin-top: 0.5em;
                            margin-left: 0.5em;
                          "
                          src="https://www.docker.com/wp-content/uploads/2022/03/Moby-logo.png"
                        />
                      </a>
                      <a href="https://carlosedubarreto.gumroad.com/l/CEB_ECON">
                        <img
                          style="
                            height: 20px;
                            outline: 1px solid grey;
                            border-radius: 5pt;
                            margin-top: 0.5em;
                            margin-left: 0.5em;
                          "
                          src="https://www.nicepng.com/png/detail/343-3436343_blender-logo-png.png"
                        />
                      </a>
        
                      <br /><br />
                      <strong>Yuliang Xiu</strong>,
                      <a href="https://is.mpg.de/~jyang">Jinlong Yang</a>,
                      <a href="https://hoshino042.github.io/homepage/">Xu Cao</a>,
                      <a href="https://ps.is.mpg.de/~dtzionas">Dimitrios Tzionas</a>,
                      <a href="https://ps.is.mpg.de/~black">Michael J. Black</a>
                      <br />
                      <i>
                        Computer Vision and Pattern Recognition 2023 (<strong
                          style="color: brown"
                          >CVPR 2023, Highlight</strong
                        >)</i
                      ><br /><br />
        
                      <a
                        shape="rect"
                        href="javascript:toggleabs(&#39;xiu2023econ&#39;)"
                        class="toggleabs"
                        ><i class="fas fa-angle-double-down" aria-hidden="true"
                          >&nbsp;</i
                        >Intro&nbsp;</a
                      >
                      <a href="https://xiuyuliang.cn/econ/"
                        ><i class="fas fa-home" aria-hidden="true">&nbsp;</i
                        >Home&nbsp;</a
                      >
                      <a href=""
                        ><i class="fa fa-file-lines" aria-hidden="true">&nbsp;</i></a
                      >[<a href="https://arxiv.org/abs/2212.07422">arXiv</a>,
                      <a href="https://readpaper.com/paper/4736821012688027649"
                        >Reader</a
                      >]
        
                      <a href=""
                        ><i class="fa fa-video" aria-hidden="true">&nbsp;</i
                        >Video&nbsp;</a
                      >[<a href="https://youtu.be/5PEd_p90kS0">En</a>,
                      <a
                        href="https://www.bilibili.com/video/BV1NM4y1B7UN/?share_source=copy_web&vd_source=5f4004d0708487723704b9e4bcf50868"
                        >中</a
                      >]
                      <a href="https://zhuanlan.zhihu.com/p/626295986"
                        ><i class="fab fa-quora">&nbsp;</i>Zhihu&nbsp;</a
                      >
                      <a
                        shape="rect"
                        href="javascript:togglebib(&#39;xiu2023econ&#39;)"
                        class="togglebib"
                        ><i class="fas fa-quote-left" aria-hidden="true">&nbsp;</i
                        >Bibtex&nbsp;</a
                      >
                      <pre
                        id="econbib"
                        xml:space="preserve"
                        style="display: none"
                      ></pre>
                      <span id="econabs" style="display: none"></span>
                    </td>
                  </tr>
                  <tr id="liao2023car">
                    <td class="pub_td1">
                      <img src="./about/car.gif" />
                    </td>
                    <td class="pub_td2">
                      <b
                        >High-Fidelity Clothed Avatar Reconstruction from a Single Image
                        &nbsp;</b
                      ><iframe
                        src="https://ghbtns.com/github-btn.html?user=TingtingLiao&repo=CAR&type=star&count=true&v=2&size=small"
                        frameborder="0"
                        scrolling="0"
                        width="100"
                        height="20"
                      ></iframe>
                      <br /><br />
                      <a href="https://github.com/TingtingLiao">Tingting Liao</a>,
                      <a
                        href="https://scholar.google.com/citations?user=-i9dCxYAAAAJ&hl=zh-CN"
                        >Xiaomei Zhang</a
                      >, <strong>Yuliang Xiu</strong>,
                      <a href="https://xyyhw.top/">Hongwei Yi</a>,
                      <a
                        href="https://scholar.google.com/citations?user=FpuBRMwAAAAJ&hl=en"
                        >Xudong Liu</a
                      >, <a href="http://maple-lab.net/gqi/">Guo-Jun Qi</a>,
                      <a href="https://yzhang2016.github.io/">Yong Zhang</a>,
                      <a href="https://xuanwangvc.github.io/">Xuan Wang</a>,
                      <a href="https://xiangyuzhu-open.github.io/homepage/"
                        >Xiangyu Zhu</a
                      >,
                      <a href="http://www.cbsr.ia.ac.cn/users/zlei/">Zhen Lei</a>
                      <br />
                      <i>
                        Computer Vision and Pattern Recognition 2023 (<strong
                          style="color: brown"
                          >CVPR 2023</strong
                        >)</i
                      ><br /><br />
        
                      <a
                        shape="rect"
                        href="javascript:toggleabs(&#39;liao2023car&#39;)"
                        class="toggleabs"
                        ><i class="fas fa-angle-double-down" aria-hidden="true"
                          >&nbsp;</i
                        >Intro&nbsp;</a
                      >
                      <a href="https://tingtingliao.github.io/CAR/"
                        ><i class="fas fa-home" aria-hidden="true">&nbsp;</i
                        >Home&nbsp;</a
                      >
                      <a href=""
                        ><i class="fa fa-file-lines" aria-hidden="true">&nbsp;</i></a
                      >[<a href="https://arxiv.org/abs/2304.03903">arXiv</a>,
                      <a href="https://readpaper.com/paper/4743346846252941313"
                        >Reader</a
                      >]
                      <a
                        shape="rect"
                        href="javascript:togglebib(&#39;liao2023car&#39;)"
                        class="togglebib"
                        ><i class="fas fa-quote-left" aria-hidden="true">&nbsp;</i
                        >Bibtex&nbsp;</a
                      >
                      <pre id="carbib" xml:space="preserve" style="display: none"></pre>
                      <span id="carabs" style="display: none"></span>
                    </td>
                  </tr>
                  <tr id="fang2022alphapose">
                    <td class="pub_td1">
                      <img src="./about/alphapose.gif" />
                    </td>
                    <td class="pub_td2">
                      <b
                        >AlphaPose: Whole-Body Regional Multi-Person Pose Estimation and
                        Tracking in Real-Time&nbsp;</b
                      ><iframe
                        src="https://ghbtns.com/github-btn.html?user=MVIG-SJTU&repo=AlphaPose&type=star&count=true&v=2&size=small"
                        frameborder="0"
                        scrolling="0"
                        width="200"
                        height="20"
                      ></iframe>
                      <br /><br />
                      <a href="https://fang-haoshu.github.io/">Hao-shu Fang*</a>,
                      <a href="https://jeffli.site/">Jiefeng Li*</a>, Hongyang Tang,
                      <a href="https://www.isdas.cn/">Chao Xu</a>, Haoyi Zhu,
                      <strong>Yuliang Xiu</strong>,
                      <a href="https://dirtyharrylyl.github.io/">Yong-lu Li</a>,
                      <a href="https://www.mvig.org/">Cewu Lu</a>,
                      <br />
                      <i
                        >IEEE Transactions on Pattern Analysis and Machine Intelligence
                        (<strong style="color: brown">TPAMI 2022</strong>)</i
                      ><br /><br />
        
                      <a
                        shape="rect"
                        href="javascript:toggleabs(&#39;fang2022alphapose&#39;)"
                        class="toggleabs"
                        ><i class="fas fa-angle-double-down" aria-hidden="true"
                          >&nbsp;</i
                        >Intro&nbsp;</a
                      >
                      <a href="https://github.com/MVIG-SJTU/AlphaPose"
                        ><i class="fas fa-home" aria-hidden="true">&nbsp;</i
                        >Home&nbsp;</a
                      >
                      <a href=""
                        ><i class="fa fa-file-lines" aria-hidden="true">&nbsp;</i></a
                      >[<a href="https://arxiv.org/abs/2211.03375">arXiv</a>,
                      <a href="https://readpaper.com/paper/4687536320180928513"
                        >Reader</a
                      >]
                      <a href="https://www.youtube.com/watch?v=Z2WPd59pRi8&t=4s"
                        ><i class="fa fa-video" aria-hidden="true">&nbsp;</i
                        >Video&nbsp;</a
                      >
                      <a
                        shape="rect"
                        href="javascript:togglebib(&#39;fang2022alphapose&#39;)"
                        class="togglebib"
                        ><i class="fas fa-quote-left" aria-hidden="true">&nbsp;</i
                        >Bibtex&nbsp;</a
                      >
                      <pre
                        id="alphaposebib"
                        xml:space="preserve"
                        style="display: none"
                      ></pre>
                      <span id="alphaposeabs" style="display: none"></span>
                    </td>
                  </tr>
        
                  <tr id="gao2022dart">
                    <td class="pub_td1">
                      <marquee behavior="alternate" direction="left" scrollamount="3">
                        <img src="./about/dart.png" height="120" width="160" />
                      </marquee>
                    </td>
                    <td class="pub_td2">
                      <b
                        >DART: Articulated Hand Model with Diverse Accessories and Rich
                        Textures&nbsp;</b
                      ><iframe
                        src="https://ghbtns.com/github-btn.html?user=DART2022&repo=DART&type=star&count=true&v=2&size=small"
                        frameborder="0"
                        scrolling="0"
                        width="100"
                        height="20"
                      ></iframe>
                      <br /><br />
                      <a href="https://tomguluson92.github.io/">Daiheng Gao*</a>,
                      <strong>Yuliang Xiu*</strong>,
                      <a href="https://kailinli.top/">Kailin Li*</a>,
                      <a href="https://lixiny.github.io/">Lixin Yang*</a>, Feng Wang,
                      Peng Zhang, Bang Zhang,
                      <a href="https://www.mvig.org/">Cewu Lu</a>,
                      <a href="https://www.cs.sfu.ca/~pingtan/">Ping Tan</a>
                      <br />
                      <i
                        >Neural Information Processing Systems (<strong
                          style="color: brown"
                          >NeurIPS 2022</strong
                        >
                        <i style="color: brown">-Datasets and Benchmarks Track</i>)</i
                      ><br /><br />
        
                      <a
                        shape="rect"
                        href="javascript:toggleabs(&#39;gao2022dart&#39;)"
                        class="toggleabs"
                        ><i class="fas fa-angle-double-down" aria-hidden="true"
                          >&nbsp;</i
                        >Intro&nbsp;</a
                      >
                      <a href="https://dart2022.github.io/"
                        ><i class="fas fa-home" aria-hidden="true">&nbsp;</i
                        >Home&nbsp;</a
                      >
                      <a href=""
                        ><i class="fa fa-file-lines" aria-hidden="true">&nbsp;</i></a
                      >[<a href="https://arxiv.org/abs/2210.07650">arXiv</a>,
                      <a href="https://readpaper.com/paper/4679562097659494401"
                        >Reader</a
                      >]
                      <a href="https://youtu.be/VvlUYe-9b7U"
                        ><i class="fa fa-video" aria-hidden="true">&nbsp;</i
                        >Video&nbsp;</a
                      >
                      <a
                        shape="rect"
                        href="javascript:togglebib(&#39;gao2022dart&#39;)"
                        class="togglebib"
                        ><i class="fas fa-quote-left" aria-hidden="true">&nbsp;</i
                        >Bibtex&nbsp;</a
                      >
                      <pre
                        id="dartbib"
                        xml:space="preserve"
                        style="display: none"
                      ></pre>
                      <span id="dartabs" style="display: none"></span>
                    </td>
                  </tr>
        
                  <tr id="xiu2021icon" class="focus">
                    <td class="pub_td1">
                      <img src="./about/icon.jpeg" />
                    </td>
                    <td class="pub_td2">
                      <b>ICON: Implicit Clothed humans Obtained from Normals&nbsp;</b>
                      <iframe
                        src="https://ghbtns.com/github-btn.html?user=yuliangxiu&repo=ICON&type=star&count=true&v=2&size=small"
                        frameborder="0"
                        scrolling="0"
                        width="120"
                        height="20"
                      ></iframe
                      ><a
                        href="https://huggingface.co/spaces/Yuliang/ICON"
                        style="padding-left: 0.5rem"
                        ><img
                          style="margin-top: 0.5em"
                          src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-ICON-orange"
                      /></a>
                      <a
                        href="https://colab.research.google.com/drive/1-AWeWhPvCTBX0KfMtgtMk10uPU05ihoA?usp=sharing"
                        style="padding-left: 0.5rem"
                        ><img
                          style="margin-top: 0.5em"
                          src="https://colab.research.google.com/assets/colab-badge.svg"
                          alt="Google Colab" /></a
                      ><br /><br />
                      <strong>Yuliang Xiu</strong>,
                      <a href="https://is.mpg.de/~jyang">Jinlong Yang</a>,
                      <a href="https://ps.is.mpg.de/~dtzionas">Dimitrios Tzionas</a>,
                      <a href="https://ps.is.mpg.de/~black">Michael J. Black</a>
                      <br />
                      <i
                        >Computer Vision and Pattern Recognition 2022 (<strong
                          style="color: brown"
                          >CVPR 2022</strong
                        >)</i
                      ><br /><br />
        
                      <a
                        shape="rect"
                        href="javascript:toggleabs(&#39;xiu2021icon&#39;)"
                        class="toggleabs"
                        ><i class="fas fa-angle-double-down" aria-hidden="true"
                          >&nbsp;</i
                        >Intro&nbsp;</a
                      >
                      <a href="https://icon.is.tue.mpg.de"
                        ><i class="fas fa-home" aria-hidden="true">&nbsp;</i
                        >Home&nbsp;</a
                      >
                      <a href=""
                        ><i class="fa fa-file-lines" aria-hidden="true">&nbsp;</i></a
                      >[<a href="https://arxiv.org/abs/2112.09127">arXiv</a>,
                      <a href="https://readpaper.com/paper/4569785684533977089"
                        >Reader</a
                      >]
                      <a
                        href="https://www.icloud.com.cn/keynote/01dxNGQJ8mBIIUG6i8O61_UCA#ICON-CVPR2022"
                        ><i class="fas fa-chalkboard-user" aria-hidden="true">&nbsp;</i
                        >Slides&nbsp;</a
                      >
                      <a href=""
                        ><i class="fa fa-video" aria-hidden="true">&nbsp;</i
                        >Video&nbsp;</a
                      >[<a href="https://youtu.be/hZd6AYin2DE">En</a>,
                      <a href="https://jmq.xet.tech/s/4qkfFu">中</a>]
                      <a href="https://zhuanlan.zhihu.com/p/477379718"
                        ><i class="fab fa-quora">&nbsp;</i>Zhihu&nbsp;</a
                      >
                      <a
                        shape="rect"
                        href="javascript:togglebib(&#39;xiu2021icon&#39;)"
                        class="togglebib"
                        ><i class="fas fa-quote-left" aria-hidden="true">&nbsp;</i
                        >Bibtex&nbsp;</a
                      >
                      <pre
                        id="iconbib"
                        xml:space="preserve"
                        style="display: none"
                      ></pre>
                      <span id="iconabs" style="display: none"></span>
                    </td>
                  </tr>
        
                  <tr id="li2020monoport">
                    <td class="pub_td1">
                      <img src="./about/monoport.png" />
                    </td>
                    <td class="pub_td2">
                      <b>Monocular Real-Time Volumetric Performance Capture&nbsp;</b
                      ><iframe
                        src="https://ghbtns.com/github-btn.html?user=Project-Splinter&repo=MonoPort&type=star&count=true&v=2&size=small"
                        frameborder="0"
                        scrolling="0"
                        width="170"
                        height="20"
                      ></iframe
                      ><br /><br />
                      <a href="http://www.liruilong.cn/">Ruilong Li*</a>,
                      <strong>Yuliang Xiu*</strong>,
                      <a href="https://shunsukesaito.github.io/">Shunsuke Saito</a>,
                      <a href="https://zeng.science/">Zeng Huang</a>,
                      <a href="https://kyleolsz.github.io/">Kyle Olszewski</a>,
                      <a href="https://www.hao-li.com/">Hao Li</a><br />
                      (*equal contribution)
                      <br />
                      <i
                        >European Conference on Computer Vision (<strong
                          style="color: brown"
                          >ECCV 2020</strong
                        >)</i
                      >
                      <br /><br />
                      <a
                        shape="rect"
                        href="javascript:toggleabs(&#39;li2020monoport&#39;)"
                        class="toggleabs"
                        ><i class="fas fa-angle-double-down" aria-hidden="true"
                          >&nbsp;</i
                        >Intro&nbsp;</a
                      >
                      <a href="./monoport"
                        ><i class="fas fa-home" aria-hidden="true">&nbsp;</i
                        >Home&nbsp;</a
                      >
                      <a href=""
                        ><i class="fa fa-file-lines" aria-hidden="true">&nbsp;</i></a
                      >[<a href="https://arxiv.org/abs/2007.13988">arXiv&nbsp;</a>,
                      <a href="https://readpaper.com/paper/3107197814">Reader</a>]
        
                      <a href="https://youtu.be/fQDsYVE7GtQ"
                        ><i class="fa fa-video" aria-hidden="true">&nbsp;</i
                        >Video&nbsp;</a
                      >
                      <a
                        href="https://www.zhihu.com/question/415544564/answer/1436374579"
                        ><i class="fab fa-quora">&nbsp;</i>Zhihu&nbsp;</a
                      >
                      <a
                        shape="rect"
                        href="javascript:togglebib(&#39;li2020monoport&#39;)"
                        class="togglebib"
                        ><i class="fas fa-quote-left" aria-hidden="true">&nbsp;</i
                        >Bibtex
                        <?php $paperid="9890425650052236496" ; include
                          'php/citation.php';?>&nbsp;
                      </a>
                      <pre
                        id="monoportbib"
                        xml:space="preserve"
                        style="display: none"
                      ></pre>
                      <span id="monoportabs" style="display: none"></span>
                    </td>
                  </tr>
        
                  <tr id="li2020monoport_rtl">
                    <td class="pub_td1">
                      <img src="./about/monoport.gif" />
                    </td>
                    <td class="pub_td2">
                      <b>Volumetric Human Teleportation&nbsp;</b
                      ><iframe
                        src="https://ghbtns.com/github-btn.html?user=Project-Splinter&repo=MonoPort&type=star&count=true&v=2&size=small"
                        frameborder="0"
                        scrolling="0"
                        width="170"
                        height="20"
                      ></iframe
                      ><br /><br />
                      <a href="http://www.liruilong.cn/">Ruilong Li</a>,
                      <a href="https://kyleolsz.github.io/">Kyle Olszewski</a>,
                      <strong>Yuliang Xiu</strong>,
                      <a href="https://shunsukesaito.github.io/">Shunsuke Saito</a>,
                      <a href="https://zeng.science/">Zeng Huang</a>,
                      <a href="https://www.hao-li.com/">Hao Li</a>
        
                      <br />
                      <i
                        ><strong style="color: brown"
                          >SIGGRAPH Real-Time Live 2020 (Best in Show Award)</strong
                        ></i
                      >
                      <br /><br />
                      <a
                        shape="rect"
                        href="javascript:toggleabs(&#39;li2020monoport_rtl&#39;)"
                        class="toggleabs"
                        ><i class="fas fa-angle-double-down" aria-hidden="true"
                          >&nbsp;</i
                        >Intro&nbsp;</a
                      >
                      <a href="./monoport"
                        ><i class="fas fa-home" aria-hidden="true">&nbsp;</i
                        >Home&nbsp;</a
                      >
                      <a href=""
                        ><i class="fa fa-file-lines" aria-hidden="true">&nbsp;</i></a
                      >[<a href="https://dl.acm.org/doi/abs/10.1145/3407662.3407756"
                        >Paper&nbsp;</a
                      >, <a href="https://readpaper.com/paper/3107197814">Reader</a>]
                      <a href="https://youtu.be/THxYxcEnKFk"
                        ><i class="fa fa-video" aria-hidden="true">&nbsp;</i
                        >Video&nbsp;</a
                      >
                      <a href=""
                        ><i class="fab fa-hacker-news" aria-hidden="true"></i
                        >&nbsp;Media&thinsp;(
                        <a
                          href="https://blog.siggraph.org/2020/08/broadcast-from-around-the-world-real-time-live-amazes-at-siggraph-2020.html/"
                          >1</a
                        >,
                        <a
                          href="https://www.fxguide.com/fxfeatured/real-time-live-winner-monoport/"
                          >2</a
                        >,
                        <a href="https://mp.weixin.qq.com/s/Bl0HohrSVzaVPF0EHzuIWw">3</a
                        >,
                        <a
                          href="https://www.shootonline.com/news/siggraph-bestows-cg-awards-wraps-its-1st-virtual-confab/"
                          >4)</a
                        >&thinsp;</a
                      >
                      <a
                        shape="rect"
                        href="javascript:togglebib(&#39;li2020monoport_rtl&#39;)"
                        class="togglebib"
                        ><i class="fas fa-quote-left" aria-hidden="true">&nbsp;</i
                        >Bibtex
                        <?php $paperid="17210272119514281912" ; include
                            'php/citation.php';?>&nbsp;
                      </a>
                      <pre
                        id="monoportrtlbib"
                        xml:space="preserve"
                        style="display: none"
                      ></pre>
                      <span id="monoportrtlabs" style="display: none"></span>
                    </td>
                  </tr>
        
                  <tr id="xiu2018poseflow">
                    <td class="pub_td1">
                      <img src="./about/posetrack.gif" />
                    </td>
                    <td class="pub_td2">
                      <b>Pose Flow: Efficient Online Pose Tracking&nbsp;</b
                      ><iframe
                        src="https://ghbtns.com/github-btn.html?user=yuliangxiu&repo=PoseFlow&type=star&count=true&v=2&size=small"
                        frameborder="0"
                        scrolling="0"
                        width="170"
                        height="20"
                      ></iframe
                      ><br /><br />
                      <strong>Yuliang Xiu</strong>,
                      <a href="https://jeffli.site/">Jiefeng Li</a>,
                      <a href="https://why2011btv.github.io">Haoyu Wang</a>, Yinghong
                      Fang, <a href="http://mvig.org">Cewu Lu</a>
        
                      <br />
                      <i
                        >British Machine Vision Conference (<strong style="color: brown"
                          >BMVC 2018</strong
                        >)</i
                      >
                      <br /><br />
                      <a
                        shape="rect"
                        href="javascript:toggleabs(&#39;xiu2018poseflow&#39;)"
                        class="toggleabs"
                        ><i class="fas fa-angle-double-down" aria-hidden="true"
                          >&nbsp;</i
                        >Intro&nbsp;</a
                      >
                      <a href=""
                        ><i class="fa fa-file-lines" aria-hidden="true">&nbsp;</i></a
                      >[<a href="https://arxiv.org/abs/1802.00977">arXiv&nbsp;</a>,
                      <a href="https://readpaper.com/paper/2963708869">Reader</a>]
                      <a
                        href="https://www.zhihu.com/question/271211525/answer/554960781"
                        ><i class="fab fa-quora">&nbsp;</i>Zhihu&nbsp;</a
                      >
                      <a
                        shape="rect"
                        href="javascript:togglebib(&#39;xiu2018poseflow&#39;)"
                        class="togglebib"
                        ><i class="fas fa-quote-left" aria-hidden="true">&nbsp;</i
                        >Bibtex
                        <?php $paperid="5164850732214894798" ; include
                              'php/citation.php';?>&nbsp;
                      </a>
                      <pre
                        id="poseflowbib"
                        xml:space="preserve"
                        style="display: none"
                      ></pre>
                      <span id="poseflowabs" style="display: none"></span>
                    </td>
                  </tr>
                  <!-- <tr id="liu2018posehd">
                    <td class="pub_td1">
                      <img src="./about/tu.png" />
                    </td>
                    <td class="pub_td2">
                      <b
                        >PoseHD: Boosting Human Detectors using Human Pose
                        Information</b
                      ><br /><br />
                      <a href="http://zhijianliu.com/">Zhijian Liu</a>,
                      <a>Bowen Pan</a>, <strong>Yuliang Xiu</strong>,
                      <a href="http://mvig.org">Cewu Lu</a>
                      <br />
                      <i
                        >AAAI Conference on Artificial Intelligence (<strong
                          style="color: brown"
                          >AAAI 2018</strong
                        >)</i
                      >
                      <br /><br />
                      <a
                        shape="rect"
                        href="javascript:toggleabs(&#39;liu2018posehd&#39;)"
                        class="toggleabs"
                        ><i class="fas fa-angle-double-down" aria-hidden="true"
                          >&nbsp;</i
                        >Intro&nbsp;</a
                      >
                      <a href="https://readpaper.com/paper/2788996589"
                        ><i class="fa fa-file-lines" aria-hidden="true">&nbsp;</i
                        >Paper&nbsp;</a
                      >
                      <a
                        shape="rect"
                        href="javascript:togglebib(&#39;liu2018posehd&#39;)"
                        class="togglebib"
                        ><i class="fas fa-quote-left" aria-hidden="true">&nbsp;</i
                        >Bibtex
                        <?php $paperid="14499296953115044554" ; include
                                'php/citation.php';?>&nbsp;
                      </a>
                      <pre
                        id="posehdbib"
                        xml:space="preserve"
                        style="display: none"
                      ></pre>
                      <span id="posehdabs" style="display: none"></span>
                    </td>
                  </tr> -->
                </table>
        
                <h1>Talks</h1>
                <ul>
                  <li>
                    <strong
                      >Towards Large-scale Human Digitization: Implicit or Explicit?
                    </strong>
                    (<a
                      href="https://www.dropbox.com/s/7ncqw60nkr8g7ud/ECON%26ICON.pdf?dl=0"
                      >slides (75MB)</a
                    >,
                    <a
                      href="https://www.bilibili.com/video/BV1NM4y1B7UN/?spm_id_from=333.999.list.card_archive.click&vd_source=4fa43a1b25f1451c4212f214517d8932"
                      ><i class="fa fa-video" aria-hidden="true">&nbsp;</i
                      >Video&nbsp;(30 min)</a
                    >)
                    <details>
                      <summary>
                        <em
                          >List of talks (Taichi, PKU, CUHK, UCLA, ETH Zurich, SDU, CAS,
                          Shanghai AI Lab, BIGAI, Huawei)</em
                        >
                      </summary>
                      <ul>
                        <li>
                          <em>
                            <strong>Taichi Graphics(太极大讲堂)</strong>, hosted by
                            <a href="https://tiantianliu.cn/">Tiantian Liu</a> and
                            <a href="https://twitter.com/yanqingdw">Qing Yan</a>
                          </em>
                        </li>
                        <li>
                          <em
                            ><strong>Huawei (CG&XR武林大会)</strong>, hosted by Shiqi
                            Zhou
                          </em>
                        </li>
                        <li>
                          <em
                            ><strong>Huawei (稼先社区&藤蔓技术论坛)</strong>, hosted by
                            Tao Bai
                          </em>
                        </li>
                        <li>
                          <em
                            ><strong>ETH Zurich</strong>, Photogrammetry and Remote
                            Sensing Lab, hosted by
                            <a href="https://shengyuh.github.io/">Shengyu Huang</a>
                          </em>
                        </li>
                        <li>
                          <em>
                            <strong>BIGAI</strong>, Beijing Institute for General
                            Artifical Intelligence, hosted by
                            <a href="https://yixchen.github.io/">Yixin Chen</a>
                          </em>
                        </li>
                        <li>
                          <em>
                            <strong>Shanghai AI Laboratory</strong>, Content Generation
                            and Digitization Group, hosted by
                            <a href="https://daibo.info/">Bo Dai</a>
                          </em>
                        </li>
                        <li>
                          <em>
                            <strong>UCLA</strong>, Multi-Physics Lagrangian-Eulerian
                            Simulations (MultiPLES) Laboratory, hosted by
                            <a href="https://www.seas.upenn.edu/~ziyinq/">Ziyin Qu</a>
                          </em>
                        </li>
                        <li>
                          <em>
                            <strong>Shandong University</strong>, Interdisciplinary
                            Research Center & Taishan College, hosted by
                            <a href="https://haisenzhao.github.io/">Haisen Zhao</a>
                          </em>
                        </li>
                        <li>
                          <em>
                            <strong>Shandong University</strong>, Research Center of
                            Human-Computer Interaction and Virtual Reality, hosted by
                            <a href="https://wanglusdu.github.io/">Lu Wang</a>
                          </em>
                        </li>
                        <li>
                          <em>
                            <strong>CUHK</strong>, The Chinese University of Hong Kong
                            (Shenzhen), GAP Lab, hosted by
                            <a href="https://gaplab.cuhk.edu.cn/">Xiaoguang Han</a>
                          </em>
                        </li>
                        <li>
                          <em>
                            <strong>ICT CAS</strong>, Institute of Computing Technology,
                            Chinese Academy of Sciences, hosted by
                            <a href="http://geometrylearning.com/lin">Lin Gao</a> and
                            <a href="http://people.geometrylearning.com/~jieyang/"
                              >Jie Yang</a
                            >
                          </em>
                        </li>
                        <li>
                          <em>
                            <strong>Peking University</strong>, the School of Artificial
                            Intelligence, hosted by
                            <a href="https://siyandong.github.io/">Siyan Dong</a> and
                            <a href="https://cfcs.pku.edu.cn/baoquan/">Baoquan Chen</a>
                          </em>
                        </li>
                      </ul>
                    </details>
                  </li>
                  <li>
                    <strong>如何多快好省地重建三维数字人 </strong>(<a
                      href="https://www.techbeat.net/talk-info?id=697"
                      ><i class="fa fa-video" aria-hidden="true">&nbsp;</i
                      >TechBeat&nbsp;(40 min)</a
                    >)
                  </li>
                  <li>
                    <strong
                      >走进马克思普朗克智能系统研究所与苏黎世联邦理工AIT团队</strong
                    >
                    (<a href="https://jmq.xet.tech/s/8he6q"
                      ><i class="fa fa-video" aria-hidden="true">&nbsp;</i>&nbsp;Part
                      A&nbsp;(2h)</a
                    >,
                    <a href="https://jmq.xet.tech/s/ld2pb"
                      ><i class="fa fa-video" aria-hidden="true">&nbsp;</i>Part
                      B&nbsp;(2h)</a
                    >)
                  </li>
        
                  <li>
                    <strong>ICON: 提高三维数字人重建的姿势水平</strong> (<a
                      href="https://www.icloud.com.cn/keynote/01dxNGQJ8mBIIUG6i8O61_UCA#ICON-CVPR2022"
                      >Keynote (726MB)</a
                    >,
                    <a href="https://jmq.xet.tech/s/4qkfFu"
                      ><i class="fa fa-video" aria-hidden="true">&nbsp;</i
                      >Video&nbsp;(60 min)</a
                    >)
                  </li>
                  <li>
                    <strong>Talking Papers Podcast: ICON</strong> (<a
                      href="https://www.buzzsprout.com/1914034/10466744"
                      >Link</a
                    >)
                  </li>
                  <li>
                    <strong
                      >Towards Large-scale Avatar Creation From In-the-wild
                      Pixels</strong
                    >
                    (<a
                      href="https://www.dropbox.com/s/fhukszvkme7n17q/ICON-slides.pdf?dl=0"
                      >slides</a
                    >, 25MB)
                    <details>
                      <summary>
                        <em>List of talks (Meta, USC, OPPO, Tecent, Adobe)</em>
                      </summary>
        
                      <ul>
                        <li>
                          <em
                            ><strong>Meta Reality Labs</strong>, Pittsburgh, hosted by
                            <a href="https://shunsukesaito.github.io/"
                              >Shunsuke Saito</a
                            >
                            and
                            <a href="https://sites.google.com/view/gjnam"
                              >Giljoo Nam</a
                            ></em
                          >
                        </li>
                        <li>
                          <em
                            ><strong>USC-ICT</strong>, USC Institute for Creative
                            Technology, hosted by
                            <a href="https://www.yajie-zhao.com/">Yajie Zhao</a>
                          </em>
                        </li>
                        <li>
                          <em
                            ><strong>Adobe</strong>, Digital Humans Seminar, hosted by
                            <a href="https://zhouyisjtu.github.io/">Yi Zhou</a>
                          </em>
                        </li>
                        <li>
                          <em
                            ><strong>OPPO</strong>, US Research Center, hosted by
                            <a
                              href="https://scholar.google.com/citations?user=FpuBRMwAAAAJ&hl=en"
                              >Xudong Liu</a
                            >
                            and
                            <a
                              href="https://scholar.google.com/citations?user=Nut-uvoAAAAJ&hl=en"
                              >Guojun Qi</a
                            >
                          </em>
                        </li>
        
                        <li>
                          <em>
                            <strong>Tecent</strong>, GY-Lab(光影研究室), hosted by
                            <a href="https://www.isdas.cn/">Chao Xu</a> and
                            <a href="https://www.skicyyu.org/">Gang Yu</a>
                          </em>
                        </li>
                      </ul>
                    </details>
                  </li>
                  <li>
                    <strong
                      >Pose trajectory extraction and novel-view synthesis from visual
                      content</strong
                    >
                    (<a
                      href="https://ps.is.mpg.de/talks/pose-trajectory-extraction-and-novel-view-synthesis-from-visual-content"
                      >Link</a
                    >)
                    <ul>
                      <li>
                        <em>
                          Max Planck Institute for Intelligent Systems, hosted by
                          <a href="https://vlg.inf.ethz.ch/">Siyu Tang</a>
                        </em>
                      </li>
                    </ul>
                  </li>
                </ul>
        
                <h1>Academic Service</h1>
                <ul>
                  <li>
                    <strong>Computer Graphics (CG)</strong>: SIGGRAPH, SIGGRAPH Asia
                  </li>
                  <li>
                    <strong>Computer Vision (CV)</strong>: CVPR, ECCV, ICCV, NeurIPS,
                    T-PAMI
                  </li>
                </ul>
                <h1>Blogs</h1>
                <ul>
                  <li>
                    [论文详解]CVPR'23 (Highlight) | ECON: 一个数字人，显式隐式各自表述
                    <a href="https://zhuanlan.zhihu.com/p/626295986"
                      ><i class="fab fa-quora">&nbsp;</i
                      ><span class="fa-brands fa-zhihu fa-xl"></span
                    ></a>
                  </li>
                  <li>
                    [论文详解]CVPR 2022 | ICON: 提高三维数字人重建的姿势水平
                    <a href="https://zhuanlan.zhihu.com/p/477379718"
                      ><i class="fab fa-quora">&nbsp;</i
                      ><span class="fa-brands fa-zhihu fa-xl"></span
                    ></a>
                  </li>
                  <li>
                    [翻译]当我们谈科学研究的创新性时，我们在谈些什么
                    <a href="https://perceiving-systems.blog/en/news/novelty-in-science"
                      ><i class="fa fa-file-lines" aria-hidden="true">&nbsp;</i
                      >Source&nbsp;</a
                    >
                    <a href="./blogs/novelty.html"
                      ><i class="fas fa-language" aria-hidden="true">&nbsp;</i
                      >译文&nbsp;</a
                    >
                  </li>
                </ul>
                <div align="center">
                  <div style="width: 200px; height: 300px">
                    <!-- <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=rW3OE8b2mKOjjq_alCSlCohNv0gBVFF6R0Pj4W60HmY"></script> -->
                    <script
                      type="text/javascript"
                      src="//rf.revolvermaps.com/0/0/6.js?i=5defqxh6kgm&amp;m=1&amp;c=ff0000&amp;cr1=ffffff&amp;f=ubuntu&amp;l=0&amp;s=300&amp;bv=100&amp;hi=20"
                      async="async"
                    ></script>
                  </div>
                  <br />
                  <strong
                    >This page is last updated at
                    <span style="color: blue" id="demo"></span
                  ></strong>
                </div>
              </div>
            </div>
          </body>
        </html>
